stringr::str_extract_all(text,"\\$[,0-9]+[0-9]")[[1]]
# How about numbers of $1 billion or more
stringr::str_extract_all(text,"\\$[,0-9]{12,}[0-9]")[[1]]
# Let's try to get numbers enclosed in parentheses:
stringr::str_extract_all(text,"\\([0-9]\\)")[[1]]
# We can also use "|" to mean "or" in regular expressions:
stringr::str_extract_all(text,"a(nd|re)")[[1]]
text_split <- stringr::str_split(text,"\\n")[[1]]
text_split
# Now match on string boundaries where
#   "^" is the start of a string/line, and
#    ("$" is the end of a string/line.)
stringr::str_extract_all(text_split,"^\\(.*")
# That returned a list, and we'd probably rather have a vector
# Need to wrap this in an unlist() statement:
unlist(stringr::str_extract_all(text_split,"^\\(.*"))
one_line <- stringr::str_replace_all(text,"\\n"," ")[[1]]
one_line
item_strings <- stringr::str_extract_all(one_line,"\\(\\d\\).+?\\.")[[1]]
item_strings
for_strings <- stringr::str_match(item_strings,"For (.+), \\$")
for_strings
# We want the second column there
for_strings <- for_strings[,2]
for_strings
money_strings <- stringr::str_match(item_strings,"\\$([,\\d]+)")[,2]
money_strings
# Let's get rid of the punctuation
money_strings <- stringr::str_replace_all(money_strings,"[\\$,]","")
money_strings
# Turn them into numbers
money <- as.numeric(money_strings)
money
appropriations_data <- data.frame(for_strings,money)
appropriations_data
Tokenize_String <- function(string){
# Lowercase
temp <- tolower(string)
#' Remove everything that is not a number or letter (may want to keep more
#' stuff in your actual analyses).
temp <- stringr::str_replace_all(temp,"[^a-z0-9\\s]", " ")
# Shrink down to just one white space
temp <- stringr::str_replace_all(temp,"[\\s]+", " ")
# Split it
temp <- stringr::str_split(temp, " ")[[1]]
# Get rid of trailing "" if necessary
indices <- which(temp == "")
if (length(indices) > 0) {
temp <- temp[-indices]
}
return(temp)
}
sentence <- "The term 'data science' (originally used interchangeably with 'datalogy') has existed for over thirty years and was used initially as a substitute for computer science by Peter Naur in 1960."
clean_sentence <- Tokenize_String(sentence)
print(clean_sentence)
setwd("~/Box Sync/ISSR_Data_Management_Web_Scraping_2018/Scripts")
rm(list = ls())
# Set your working directory to some place you can find
setwd("~/Desktop")
# First we will need to install the packages we plan to use for this exercise (
# if they are not already installed on your computer).
# install.packages("httr", dependencies = TRUE)
# install.packages("stringr", dependencies = TRUE)
# httr is a package for downloading html
library(httr)
# A package for manipulating strings
library(stringr)
# Lets start by downloading an example web page:
url <- "http://www.mjdenny.com/Rcpp_Intro.html"
page <- httr::GET(url)
# As we can see, this produces a great deal of information
str(page)
# To get at the actual content of the page, we use the content() function:
page_content <- httr::content(page, "text")
# Now lets print it out
cat(page_content)
# and write it to a file for easier viewing
write.table(x = page_content,
col.names = FALSE,
row.names = FALSE,
quote = FALSE,
file = "Example_1.html")
# lets try a more complicated example page for a peice of legislation in the
# U.S. Congress
url <- "https://www.congress.gov/bill/103rd-congress/senate-bill/486/text"
# we start by using the httr package to download the source html
page <- httr::GET(url)
# as we can see, this produces a great deal of information
str(page)
# to get at the actaul content of the page, we use the content() function
page_content <- httr::content(page, "text")
# now lets print it out
cat(page_content)
# and write it to a file for easier viewing
write.table(x = page_content,
col.names = FALSE,
row.names = FALSE,
quote = FALSE,
file = "Example_2.html")
### Web Scraping Example, Part 1 ###
url <- "https://scholar.google.com/scholar?hl=en&q=https://scholar.google.com/scholar?hl=en&q=laurel+smith-doerr"
# we start by using the httr package to download the source html
page <- httr::GET(url)
# as we can see, this produces a great deal of information
str(page)
# to get at the actaul content of the page, we use the content() function
page_content <- httr::content(page, "text")
# now lets print it out
cat(page_content)
# and write it to a file for easier viewing
write.table(x = page_content,
col.names = FALSE,
row.names = FALSE,
quote = FALSE,
file = "Example_3.html")
string <- "Laurel Smith-Doerr"
return_source <- FALSE
# print out the input name
cat(string, "\n")
# make the input name all lowercase
string <- tolower(string)
# split the string on spaces
str <- stringr::str_split(string," ")[[1]]
# combine the resulting parts of the string with + signs so "Matt Denny"
# will end up as "matt+denny" which is what Google Scholar wants as input
str <- paste0(str,collapse = "+")
# add the name (which is now in the correct format) to the search querry and
# we have our web address.
str <- paste("https://scholar.google.com/scholar?hl=en&q=",str,sep = "")
# downloads the web page source code
page <- httr::GET(str)
page <- httr::content(page, "text")
### Web Scraping Example, Part 2 ###
# search for the 'Scholar</a><div id="gs_ab_md">' string which occurs
# uniquely right before google Scholar tells you how many results your
# querry returned
num_results <- str_split(page,'<div id=\\"gs_ab_md\\"><div class=\\"gs_ab_mdw\\">')[[1]][2]
# split the resulting string on the fist time you see a "(" as this will
# signify the end of the text string telling you how many results were
# returned.
num_results <- str_split(num_results,'\\(')[[1]][1]
# Print out the number of results returned by Google Scholar
cat("Querry returned", tolower(num_results), "\n")
# Look to see if the "User profiles" string is present -- grepl will return
# true if the specified text ("User profiles") is contained in the web page
# source.
if (grepl("User profiles",page)) {
# split the web page source (which is all one string) on the "Cited by "
# string and then take the second chunk of the resulting vector of
# substrings (so we can get at the number right after the first mention
# of "Cited by ")
num_cites <- str_split(page,"Cited by ")[[1]][2]
# now we want the number before the < symbol in the resulting string
# (which will be the number of cites)
num_cites <- str_split(num_cites,"<")[[1]][1]
# now let the user know how many we found
cat("Number of Cites:",num_cites,"\n")
} else {
# If we could not find the "User profiles" string, then the person
# probably does not have a profile on Google Scholar and we should let
# the user know this is the case
cat("This user may not have a Google Scholar profile \n")
}
get_google_scholar_results <- function(string,
return_source = FALSE){
# print out the input name
cat(string, "\n")
# make the input name all lowercase
string <- tolower(string)
# split the string on spaces
str <- stringr::str_split(string," ")[[1]]
# combine the resulting parts of the string with + signs so "Matt Denny"
# will end up as "matt+denny" which is what Google Scholar wants as input
str <- paste0(str,collapse = "+")
# add the name (which is now in the correct format) to the search querry and
# we have our web address.
str <- paste("https://scholar.google.com/scholar?hl=en&q=",str,sep = "")
# downloads the web page source code
page <- httr::GET(str)
page <- httr::content(page, "text")
### Web Scraping Example, Part 2 ###
# search for the 'Scholar</a><div id="gs_ab_md">' string which occurs
# uniquely right before google Scholar tells you how many results your
# querry returned
num_results <- str_split(page,'<div id=\\"gs_ab_md\\"><div class=\\"gs_ab_mdw\\">')[[1]][2]
# split the resulting string on the fist time you see a "(" as this will
# signify the end of the text string telling you how many results were
# returned.
num_results <- str_split(num_results,'\\(')[[1]][1]
# Print out the number of results returned by Google Scholar
cat("Querry returned", tolower(num_results), "\n")
# Look to see if the "User profiles" string is present -- grepl will return
# true if the specified text ("User profiles") is contained in the web page
# source.
if (grepl("User profiles",page)) {
# split the web page source (which is all one string) on the "Cited by "
# string and then take the second chunk of the resulting vector of
# substrings (so we can get at the number right after the first mention
# of "Cited by ")
num_cites <- str_split(page,"Cited by ")[[1]][2]
# now we want the number before the < symbol in the resulting string
# (which will be the number of cites)
num_cites <- str_split(num_cites,"<")[[1]][1]
# now let the user know how many we found
cat("Number of Cites:",num_cites,"\n")
} else {
# If we could not find the "User profiles" string, then the person
# probably does not have a profile on Google Scholar and we should let
# the user know this is the case
cat("This user may not have a Google Scholar profile \n")
}
# If we specified the option at the top that we wanted to return the HTML
# source, then return it, otherwise don't.
if (return_source) {
return(page)
}
}
get_google_scholar_results("Joya Misra")
get_google_scholar_results("Laurel Smith-Doerr")
get_google_scholar_results("Nilanjana Dasgupta")
page_source <- get_google_scholar_results("Reka Albert",return_source = TRUE)
install.packages(c("ROAuth","devtools","ggplot2","maps", "streamR"),
repos = "http://cran.r-project.org")
library(streamR)
# Load in your access credential we created above
setwd("~/Dropbox/Credentials/")
load("my_oauth.Rdata")
setwd("~/Desktop")
# Here we are going to use the filter function which uses dome sort of criteria
# for determining which tweets should be saved.
filterStream("tweets.json", # name of file that we want to save tweets to.
track = c("Trump", "POTUS"), # key terms to collect.
timeout = 60, # amount of time to collect data in seconds.
oauth = my_oauth) # your token!
# Load in the tweets from the tweets.json file where they were stored and turn
# them into a data.frame:
tweets.df <- parseTweets("tweets.json",
simplify = TRUE)
rm(list = ls())
setwd("~/Desktop")
library(rvest)
library(stringr)
install.packages("rvest", dependencies = TRUE)
i = 1
# get the total number of pages we will need to scrape
pages <- ceiling(2250/250)
cat("Currently working on block",i,"of",pages,"\n")
html <- read_html(paste("https://www.congress.gov/members?pageSize=250&page=",i,sep = ""))
library(rvest)
library(stringr)
html <- read_html(paste("https://www.congress.gov/members?pageSize=250&page=",i,sep = ""))
# use the tags we got off of Selectorgadget
web_page_results <- html_nodes(html, ".expanded , a")
# use the tags we got off of Selectorgadget
web_page_results <- html_nodes(html, ".expanded , a")
# get member's webpages
web_pages <- html_attr(web_page_results, "href")
# now get the member metadata
metadata_results <- html_nodes(html, ".expanded")
# parse this into a list with one entry per legislator
metadata_list <- html_children(metadata_results)
cur_metadata <- NULL
for (j in 1:(length(metadata_list)/3)) {
ind <- 3 * (j - 1) + 2
text <- paste(html_text(metadata_list[[ind]]),
html_text(metadata_list[[ind+1]]), sep = "  ")
cur_metadata <- rbind(cur_metadata,extract_metadata(text))
}
extract_metadata <- function(text) {
# replace newlines with nothing
text <- str_replace_all(text,"\\n","")
# split on two or more spaces
text <- str_split(text, "[\\s]{2,}")[[1]]
# break up name to extract chamber and name
chamber_name <- str_split(text[1]," ")[[1]]
# first is chamber
chamber <- chamber_name[1]
# rest is name
name <- paste0(chamber_name[-1], collapse = " ")
# combine together in to a vector that can be row bound together. If the
# member is a Senator, then district is NA.
if (chamber == "Senator") {
ret <- c(name, chamber,text[3], text[5], NA , text[7], text[8])
} else {
# deal with territories that do not have districts
if (length(text) == 8) {
ret <- c(name, chamber,text[3], text[5],NA, text[7], text[8])
} else {
ret <- c(name, chamber,text[3], text[7], text[5], text[9], text[10])
}
}
return(ret)
}
cur_metadata <- NULL
for (j in 1:(length(metadata_list)/3)) {
ind <- 3 * (j - 1) + 2
text <- paste(html_text(metadata_list[[ind]]),
html_text(metadata_list[[ind+1]]), sep = "  ")
cur_metadata <- rbind(cur_metadata,extract_metadata(text))
}
View(cur_metadata)
# get relevant webpages
websites <- grep("https://www.congress.gov/member/",
web_pages,
value = TRUE)
# get every other entry since there are duplicates
websites <- websites[seq(from = 1,
to = length(websites) - 1,
length.out = length(websites)/2)]
# add on web pages
cur_metadata <- cbind(cur_metadata,websites)
# add everything onto the full metadata file
legislator_data <- rbind(legislator_data, cur_metadata)
# create a null object to append results to
legislator_data <- NULL
# add everything onto the full metadata file
legislator_data <- rbind(legislator_data, cur_metadata)
View(legislator_data)
legislator_data <- data.frame(Name = legislator_data [,1],
Position = legislator_data [,2],
State = legislator_data [,3],
District = legislator_data [,5],
Party = legislator_data [,4],
Years_of_Service = legislator_data [,6],
Other_Chamber_YoS = legislator_data [,7],
URL = legislator_data [,8],
stringsAsFactors = FALSE)
additional_legislator_data <- NULL
i = 1
cat("Currently working on legislator",i,"of",nrow(legislator_data),"\n")
html <- read_html(legislator_data$URL[i])
# use the tags we got off of Selectorgadget to find metadata on bills that
# legislator introduced and statistics about them
metadata_results <- html_nodes(html, "#content :nth-child(1)")
# get all of the metadata items in a list object
metadata_list <- html_children(metadata_results)
for (j in 1:length(metadata_list)) {
# break the first time we encounter this string
if  (grepl("collapseSponsorship",html_text(metadata_list[[j]]))) {
text <- html_text(metadata_list[[j]])
break
}
}
birthdate <- html_nodes(html, ".birthdate")
birthdate <- html_text(birthdate)
bio_link <- html_nodes(html, ".member_bio_link")
bio_link <- html_attr( bio_link, "href")
text
# find information about the bills that legislator introduced.
for (j in 1:length(metadata_list)) {
# break the first time we encounter this string
if  (grepl("collapseSponsorship",html_text(metadata_list[[j]]))) {
text <- html_text(metadata_list[[j]])
cat("Found!")
break
}
}
legislator_data$URL[i]
for (j in 1:length(metadata_list)) {
# break the first time we encounter this string
if  (grepl("#facetItemsponsorshipSponsored_Legislation",html_text(metadata_list[[j]]))) {
text <- html_text(metadata_list[[j]])
cat("Found!")
break
}
}
for (j in 1:length(metadata_list)) {
# break the first time we encounter this string
if  (grepl("Sponsored_Legislation",html_text(metadata_list[[j]]))) {
text <- html_text(metadata_list[[j]])
cat("Found!")
break
}
}
web_page_results <- html_nodes(html, ".expanded , a")
# get member's webpages
web_pages <- html_attr(web_page_results, "href")
web_pages
# use the tags we got off of Selectorgadget to find metadata on bills that
# legislator introduced and statistics about them
metadata_results <- html_nodes(html, "#content :nth-child(1)")
# use the tags we got off of Selectorgadget to find metadata on bills that
# legislator introduced and statistics about them
metadata_results <- html_nodes(html, ".expanded-view")
# get all of the metadata items in a list object
metadata_list <- html_children(metadata_results)
j = 1
text <- html_text(metadata_list[[j]])
text
metadata_results <- html_nodes(html, "#facetItemsponsorshipSponsored_Legislation")
# get all of the metadata items in a list object
metadata_list <- html_children(metadata_results)
text <- html_text(metadata_list[[1]])
text <- html_text(metadata_list[[2]])
install.packages("quanteda", dependencies = TRUE)
require(stringr)
require(quanteda)
setwd("~/Box Sync/ISSR_Data_Management_Web_Scraping_2018/Scripts")
# set working directory (you will need to change this for your computer)
setwd("~/Box Sync/ISSR_Data_Management_Web_Scraping_2018/Data/Bill_Text")
documents <- rep("", length = 100)
# loop over documents
for (i in 1:100) {
cat("currently working on bill:",i,"\n")
# set the current bill number
ind <- 97000 + i
# get the text of the bill
text <- readLines(paste("Bill_",ind,".txt", sep = ""))
# collapse it together into a string and store it in a vector
documents[i] <- paste0(text,collapse = " ")
}
doc_term_matrix <- quanteda::dfm(documents,
tolower = TRUE,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_separators = TRUE,
remove_twitter = FALSE,
stem = TRUE)
head(doc_term_matrix@Dimnames$features, n = 100)
word_counts <- colSums(doc_term_matrix)
# order word counts
word_counts <- word_counts[order(word_counts, decreasing = TRUE)]
# top words
head(word_counts,n = 100)
# bottom words
tail(word_counts,n = 20)
corp <- quanteda::data_corpus_inaugural
summary(corp)
texts(data_corpus_inaugural)[1]
options(width=120)
kwic(data_corpus_inaugural, "humble", window=4)
kwic(data_corpus_inaugural, "tombstones", window=4)
options(width=80)
doc_term_matrix <- quanteda::dfm(corp,
tolower = TRUE,
stem = FALSE,
remove_punct = TRUE,
remove = stopwords("english"),
ngrams = 1)
# let's take a look at some vocabulary terms:
# What kind of object is doc_term_matrix?
class(doc_term_matrix)
# How big is it? How sparse is it?
doc_term_matrix
# Let's look inside it a bit:
doc_term_matrix[1:5,1:5]
# What are the most frequent terms?
topfeatures(doc_term_matrix,40)
# Besides "tombstones," what other words were never used in an inaugural before Trump?
unique_to_trump <- as.vector(colSums(doc_term_matrix) == doc_term_matrix["2017-Trump",])
colnames(doc_term_matrix)[unique_to_trump]
textplot_wordcloud(doc_term_matrix,
min.freq = 100,
random.order = FALSE,
rot.per = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
textplot_wordcloud(doc_term_matrix,
min_count = 100,
colorrandom_order = FALSE,
rotation = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
warnings()
textplot_wordcloud(doc_term_matrix,
min_count = 100,
color_random_order = FALSE,
rotation = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
textplot_wordcloud(doc_term_matrix,
min_count = 100,
random.order = FALSE,
rotation = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
warnings()
textplot_wordcloud(doc_term_matrix,
min_count = 100,
rotation = .25,
colors = RColorBrewer::brewer.pal(8,"Dark2"))
textplot_wordcloud(doc_term_matrix,
min_count = 100,
rotation = .25,
color = RColorBrewer::brewer.pal(8,"Dark2"))
textplot_wordcloud(doc_term_matrix["2017-Trump",],
min_count = 3,
random_order = FALSE,
rotation = .25,
color = RColorBrewer::brewer.pal(8,"Dark2"))
set.seed(100)
textplot_wordcloud(doc_term_matrix["2017-Trump",],
min_count = 3,
random_order = FALSE,
rotation = .25,
color = RColorBrewer::brewer.pal(8,"Dark2"))
doc_term_matrix <- quanteda::dfm(corp,
tolower = FALSE,
stem = TRUE,
remove_punct = FALSE,
remove = stopwords("english"),
ngrams = 1)
# How big is it now? How sparse is it now?
doc_term_matrix
# What are the most frequent terms?
topfeatures(doc_term_matrix,40)
doc_term_matrix <- quanteda::dfm(corp,
tolower = TRUE,
stem = FALSE,
remove_punct = TRUE,
remove = stopwords("english"),
ngrams = 2)
# How big is it now? How sparse is it now?
doc_term_matrix
# What are the most frequent terms?
topfeatures(doc_term_matrix,40)
